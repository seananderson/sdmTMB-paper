\clearpage

# Example: spatiotemporal modelling of Pacific cod {#app:pcod}

```{r setup-pcod, include = FALSE, cache=FALSE}
knitr::opts_chunk$set(
  echo = TRUE
)
```

```{r packages, message=FALSE, warning=FALSE, cache=FALSE}
library("ggplot2")
library("dplyr")
library("sdmTMB")
theme_set(theme_minimal())
```

We will fit a geostatistical spatiotemporal model with \pkg{sdmTMB} for the purposes of index standardization. We will use a data set built into the package: trawl survey data for Pacific Cod in Queen Charlotte Sound. The density units are kg/km^2^. Here, X and Y are UTM zone 9 coordinates in kilometres. If our spatial coordinates were not already in an equidistant projection, we would transform them [e.g., using `sf::st_transform()`, @pebesma2018] to ensure that distance remains constant throughout the study region or using the built-in `sdmTMB::add_utm_columns()` function.

First, we construct a mesh to approximate the spatial process, which is used in the SPDE calculations. The `make_mesh()` function accepts the data coordinates and the minimum allowed distance between vertices (`cutoff`) or approximate number of knots in the mesh (`n_knots`). Custom built meshes with \pkg{INLA} (including meshes with barriers limiting covariance across polygons) can also be passed in. See examples included in `?make_mesh`.

For example, a mesh with a cutoff distance of 10 km can be made (Fig.\ \ref{fig:plot-mesh}).

```{r plot-mesh, fig.align='center', fig.asp=1, out.width="3in", echo=TRUE, fig.cap="Delaunay triangulation mesh.", fig.pos='ht', fig.align='center'}
mesh <- make_mesh(pcod, xy_cols = c("X", "Y"), cutoff = 10)
plot(mesh)
```

Next we will fit a GLMM. For this example, we will include `0 + as.factor(year)` so that there is a factor predictor that represents the mean estimate for each time slice. Setting `silent = FALSE` would gives us optimization details as the model fits.

An alternative would be to model catch as the response with an offset (`offset` argument) for log area swept.

```{r fit-pcod, warning=FALSE, message=FALSE, cache=TRUE, echo=TRUE, results='hide'}
fit <- sdmTMB(
  density ~ 0 + as.factor(year),
  data = pcod,
  time = "year", 
  mesh = mesh,
  family = tweedie(link = "log"),
  silent = FALSE
)
```

Print the model to inspect the fit:

```{r print-fit, echo=TRUE}
fit
```

Extract the parameter estimates as a data frame:

```{r pcod-tidy, echo=TRUE}
tidy(fit, conf.int = TRUE)
tidy(fit, effects = "ran_pars", conf.int = TRUE)
```

We can predict on the original data locations:

```{r pcod-predict, cache=TRUE, echo=TRUE}
pred <- predict(fit)
```

We can inspect simulated residuals from the DHARMa package (Fig.\ \ref{fig:sim-resids}).

```{r sim-resids, out.width="4in", warning=FALSE, results='hide', cache=TRUE, echo=TRUE, fig.cap="Simulated residuals from the DHARMa package.", fig.pos='ht', fig.align='center'}
set.seed(123)
simulate(fit, nsim = 500) %>%
  dharma_residuals(fit)
```

Or perhaps better, use MCMC residuals (Fig.\ \ref{fig:mcmc-resids-vis}).

```{r mcmc-resids, warning=FALSE, results='hide', cache=TRUE}
set.seed(123)
r <- residuals(fit,
  type = "mle-mcmc", 
  mcmc_iter = 200, mcmc_warmup = 100
)
```

```{r mcmc-resids-vis, out.width="4in", fig.cap="MCMC residuals.", fig.pos='ht', fig.align='center'}
qqnorm(r)
qqline(r)
```

Next, we want to predict on a fine-scale grid over the entire survey domain so we can sum up the expected biomass across the entire region each year. There is a grid built into the \pkg{sdmTMB} package for Queen Charlotte Sound (named `qcs_grid`). Our prediction grid also needs to have all the covariates that we used in the model above. In this case, that is a `year` column as well as `X` and `Y`. We will replicate the internal spatial grid over all years (calling it `survey_grid`) and predict on it.

```{r predict-newdata}
survey_grid <- replicate_df(
  qcs_grid, 
  time_name = "year", 
  time_values = unique(pcod$year)
)

pred_qcs <- predict(fit, newdata = survey_grid)
```

Next we will make a small function to help plot maps.

```{r plot-map}
plot_map <- function(dat, column) {
  ggplot(dat, aes_string("X", "Y", fill = column)) +
    geom_raster() +
    facet_wrap(~year) +
    coord_fixed()
}
```

There are four kinds of predictions that we get out of the model. First we will show the predictions that incorporate all fixed effects and random effects (Fig.\ \ref{fig:plot-all-effects}). 

```{r plot-all-effects, fig.cap="Model predictions based on all fixed effects and random effects.", fig.pos='ht', fig.align='center'}
plot_map(pred_qcs, "exp(est)") +
  # ggtitle("Prediction (fixed effects + all random effects)") +
  scale_fill_viridis_c(trans = "log10") 
```

We can also look at just the fixed effects, here `year` (Fig.\ \ref{fig:plot-fix-defects}).

```{r plot-fix-defects, fig.cap="Predictions based on only the non-random field elements.", fig.pos='ht', fig.align='center'}
plot_map(pred_qcs, "exp(est_non_rf)") +
  # ggtitle("Prediction (fixed effects only)") +
  scale_fill_viridis_c(trans = "sqrt")
```

We can look at the spatial random effects that represent consistent deviations in space through time that are not accounted for by our fixed effects (Fig.\ \ref{fig:plot-spatial-effects}). In other words, these deviations represent consistent biotic and abiotic factors that are affecting biomass density but are not accounted for in the model.

```{r plot-spatial-effects, fig.cap="Spatial random effects only.", fig.pos='ht', fig.align='center'}
plot_map(pred_qcs, "omega_s") +
  # ggtitle("Spatial random effects only") +
  scale_fill_gradient2()
```

Finally, we can look at the spatiotemporal random effects that represent deviations from the fixed effect predictions and the spatial random effect deviations (Fig.\ \ref{fig:plot-spatiotemporal-effects}). These represent biotic and abiotic factors causing spatial correlation that are changing through time and are not accounted for in the model.

```{r plot-spatiotemporal-effects, fig.cap="Spatiotemporal random effects only.", fig.pos='ht', fig.align='center'}
plot_map(pred_qcs, "epsilon_st") +
  # ggtitle("Spatiotemporal random effects only") +
  scale_fill_gradient2()
```

The fastest way to assess uncertainty on our spatial predictions is with simulation from the joint precision matrix.

```{r pcod-sims, cache=TRUE}
pred_sims <- predict(fit, newdata = survey_grid, nsim = 200)
dim(pred_sims)
```

This returns a matrix with a row for each row of the `newdata` data frame and a column for each replicate draw. We can use this matrix to calculate the standard deviation or coefficient of variation (Fig.\ \ref{fig:sims-cv-plot}).

```{r sims-cv-plot, cache=TRUE, fig.cap="Spatiotemporal coefficient of variation.", fig.pos='ht', fig.align='center'}
survey_grid$cv <- apply(pred_sims, 1, function(x) sd(exp(x)) / mean(exp(x)))
ggplot(survey_grid, aes(X, Y, fill = cv)) +
  geom_raster() +
  facet_wrap(~year) +
  coord_fixed() +
  scale_fill_viridis_c(trans = "log10", option = "D")
```

We can calculate a standardized population index by predicting on our survey domain grid and setting `return_tmb_object = TRUE` (Fig.\ \ref{fig:pcod-app-index}). This returns the \pkg{TMB} objective function, which is required to pass the output to `get_index()`.

```{r pcod-app-index, fig.asp=0.45, out.width="5in", cache=TRUE, fig.cap="Geostatistical population index.", fig.pos='ht', fig.align='center'}
survey_grid$area <- 4 # all 2 x 2km
pred2 <- predict(
  fit,
  newdata = survey_grid, 
  return_tmb_object = TRUE
)
ind <- get_index(pred2, area = survey_grid$area, bias_correct = TRUE)

ggplot(ind, aes(year, est / 1000)) +
  geom_line() +
  geom_ribbon(aes(ymin = lwr / 1000, ymax = upr / 1000), alpha = 0.4) +
  ylab("Biomass (t)")
```

The above example uses `bias_correct = TRUE` in `get_index()`. The `bias_correct` argument is a feature of TMB for models with random effects, and more details can be found in @thorson2016bias. Leaving `bias_correct = FALSE` is fine for experimentation, but will be biased due to the nonlinear (`exp()`) transformation of the random effects. Setting `bias_correct = TRUE` implements a generic bias correction algorithm that should correct for this, but takes more time and memory than skipping the bias correction.
